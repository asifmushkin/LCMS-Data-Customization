{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of keycode from mdb's into excel sheets\n",
    "* Extraction of keycode and gps in excel file from mdb's\n",
    "* Extraction of distress in excel file from keycode\n",
    "* Extraction of remarks in excel file from keycode\n",
    "* Converting distress excel into table\n",
    "* Creating point and area distress's table from distress table\n",
    "* Creating point and line feature classes of distress's\n",
    "* Reprojection of created feature classes into UTM coordinates reference system \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "                  Survey_ID  Chainage      SPEED  LRP_Number  LRP_OFFSET  \\\n",
      "1159  CL_ADI_119_5_RO5-TL-1       0.0  65.500000           0         0.0   \n",
      "1160  CL_ADI_119_5_RO5-TL-1       1.0  65.500000           0         1.0   \n",
      "1161  CL_ADI_119_5_RO5-TL-1       2.0  65.500000           0         2.0   \n",
      "1162  CL_ADI_119_5_RO5-TL-1       3.0  65.500000           0         3.0   \n",
      "1163  CL_ADI_119_5_RO5-TL-1       4.0  65.500000           0         4.0   \n",
      "1164  CL_ADI_119_5_RO5-TL-1       5.0  65.500000           0         5.0   \n",
      "1165  CL_ADI_119_5_RO5-TL-1       6.0  65.500000           0         6.0   \n",
      "1166  CL_ADI_119_5_RO5-TL-1       7.0  65.400002           0         7.0   \n",
      "1167  CL_ADI_119_5_RO5-TL-1       8.0  65.400002           0         8.0   \n",
      "1168  CL_ADI_119_5_RO5-TL-1       9.0  65.400002           0         9.0   \n",
      "\n",
      "          GPS_TIME   Latitude  Longitude   Northing    Easting  Alt_HAE  \\\n",
      "1159  07:19:34.651  24.467646  54.395432  24.467646  54.395432      0.0   \n",
      "1160  07:19:34.706  24.467654  54.395428  24.467654  54.395428      0.0   \n",
      "1161  07:19:34.761  24.467662  54.395423  24.467662  54.395423      0.0   \n",
      "1162  07:19:34.816  24.467670  54.395418  24.467670  54.395418      0.0   \n",
      "1163  07:19:34.872  24.467678  54.395413  24.467678  54.395413      0.0   \n",
      "1164  07:19:34.927  24.467686  54.395408  24.467686  54.395408      0.0   \n",
      "1165  07:19:34.982  24.467693  54.395403  24.467693  54.395403      0.0   \n",
      "1166  07:19:35.037  24.467701  54.395398  24.467701  54.395398      0.0   \n",
      "1167  07:19:35.092  24.467709  54.395394  24.467709  54.395394      0.0   \n",
      "1168  07:19:35.147  24.467717  54.395389  24.467717  54.395389      0.0   \n",
      "\n",
      "      Alt_MSL     HEADING  PDOP  HDOP   GPS_SOURCE   nID  \\\n",
      "1159     4.25  330.113007   1.2   0.6  DGPS-Geo-DR  1160   \n",
      "1160     4.25  330.087006   1.2   0.6  DGPS-Geo-DR  1161   \n",
      "1161     4.25  330.071991   1.2   0.6  DGPS-Geo-DR  1162   \n",
      "1162     4.25  330.049988   1.2   0.6  DGPS-Geo-DR  1163   \n",
      "1163     4.25  330.036987   1.2   0.6  DGPS-Geo-DR  1164   \n",
      "1164     4.25  330.020996   1.2   0.6  DGPS-Geo-DR  1165   \n",
      "1165     4.25  330.022003   1.2   0.6  DGPS-Geo-DR  1166   \n",
      "1166     4.25  330.018005   1.2   0.6  DGPS-Geo-DR  1167   \n",
      "1167     4.25  330.024994   1.2   0.6  DGPS-Geo-DR  1168   \n",
      "1168     4.25  330.035004   1.2   0.6  DGPS-Geo-DR  1169   \n",
      "\n",
      "                Survey_ID_1  \n",
      "1159  CL_ADI_119_5_RO5-TL-1  \n",
      "1160  CL_ADI_119_5_RO5-TL-1  \n",
      "1161  CL_ADI_119_5_RO5-TL-1  \n",
      "1162  CL_ADI_119_5_RO5-TL-1  \n",
      "1163  CL_ADI_119_5_RO5-TL-1  \n",
      "1164  CL_ADI_119_5_RO5-TL-1  \n",
      "1165  CL_ADI_119_5_RO5-TL-1  \n",
      "1166  CL_ADI_119_5_RO5-TL-1  \n",
      "1167  CL_ADI_119_5_RO5-TL-1  \n",
      "1168  CL_ADI_119_5_RO5-TL-1  \n",
      "Successfully completed!!\n"
     ]
    }
   ],
   "source": [
    "import pyodbc,os,pandas as pd\n",
    "arcpy.env.workspace = r\"D:\\_1.Project\\AACM\\Pavement\\GDB\\Distress Data Extraction.gdb\"\n",
    "mdb_directory = r\"D:\\_1.Project\\AACM\\Pavement\\Data\\DataView MDB's\"\n",
    "o_gps = r\"D:\\_1.Project\\AACM\\Pavement\\Excel Workbook\\Distress_Workbook\\E_GPS.xlsx\"\n",
    "o_keycode = r\"D:\\_1.Project\\AACM\\Pavement\\Excel Workbook\\Distress_Workbook\\E_Keycode.xlsx\"\n",
    "o_distress = r\"D:\\_1.Project\\AACM\\Pavement\\Excel Workbook\\Distress_Workbook\\E_distress_keycode.xlsx\"\n",
    "o_distress_remarks = r\"D:\\_1.Project\\AACM\\Pavement\\Excel Workbook\\Distress_Workbook\\E_distress_Remarks.xlsx\"\n",
    "#defining function to read mdb's\n",
    "def read_mdb(x):\n",
    "    mdb_files = []\n",
    "    for root, dirs, files in os.walk(x):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mdb\"):\n",
    "                mdb_files.append(os.path.join(root, file))\n",
    "    return mdb_files\n",
    "\n",
    "#defining function to read tables in mdb\n",
    "def read_tables_by_prefix(mdb_file, table_prefix):\n",
    "    conn_str = f\"Driver={{Microsoft Access Driver (*.mdb, *.accdb)}};Dbq={mdb_file};\"\n",
    "    connection = pyodbc.connect(conn_str)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    tables = []\n",
    "    for table_info in cursor.tables(tableType='TABLE'):\n",
    "        table_name = table_info.table_name\n",
    "        if table_name.startswith(table_prefix):\n",
    "            tables.append(table_name)\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    for table in tables:\n",
    "        query = f\"SELECT * FROM [{table}]\"\n",
    "        table_data = pd.read_sql(query, connection)\n",
    "        table_data['Survey_ID_1'] = os.path.splitext(os.path.basename(mdb_file))[0]\n",
    "        data = pd.concat([data, table_data], ignore_index=True)\n",
    "\n",
    "    connection.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "#calling function to read mdb's\n",
    "mdb_files = read_mdb(mdb_directory)\n",
    "print(len(mdb_files))\n",
    "\n",
    "\n",
    "#pandas data frame for storing extracted gps and keycode\n",
    "GPS_P = pd.DataFrame()\n",
    "DV_keycode = pd.DataFrame()\n",
    "\n",
    "\n",
    "#extracting and merging gps and keycode from mdb's \n",
    "for mdb_file in mdb_files:\n",
    "    dt_vedio = read_tables_by_prefix(mdb_file, \"dt_VideoKeyCode_Raw\")\n",
    "    gps = read_tables_by_prefix(mdb_file, \"dt_GPS_Processed\")\n",
    "\n",
    "    DV_keycode = pd.concat([DV_keycode, dt_vedio], ignore_index=True)\n",
    "    GPS_P = pd.concat([GPS_P, gps], ignore_index=True)\n",
    "\n",
    "    \n",
    "#filtering data frame to remove calibration rows from pandas data frame    \n",
    "GPS_P_filtered = GPS_P[~GPS_P['Survey_ID'].str.startswith('CALIBRATION')]\n",
    "DV_keycode_filtered = DV_keycode[~DV_keycode['SURVEY_ID'].str.startswith('CALIBRATION')]\n",
    "\n",
    "\n",
    "#exporting gps and keycode from pandas data frame\n",
    "GPS_P_filtered.to_excel(o_gps, index=False)\n",
    "DV_keycode_filtered.to_excel(o_keycode, index = False)\n",
    "\n",
    "\n",
    "#creating two data frame for distress and remarks\n",
    "field = 'EVENT_DESC'\n",
    "field_values = ['REMARKS','PAVEMENT MARKING']\n",
    "distress = DV_keycode[~DV_keycode[field].isin(field_values)]\n",
    "distress_Remark = DV_keycode[DV_keycode[field].isin(field_values)]\n",
    "\n",
    "#exporting distress and remarks excel from pandas data frame\n",
    "distress.to_excel(o_distress, index = False)\n",
    "distress_Remark.to_excel(o_distress_remarks, index = False) \n",
    "\n",
    "#converting distress and remarks excel workbooks into tables\n",
    "arcpy.ExcelToTable_conversion(r\"D:\\_1.Project\\AACM\\Pavement\\Excel Workbook\\Distress_Workbook\\E_distress_keycode.xlsx\", \"Keycode_tbl\", \"Sheet1\")\n",
    "arcpy.ExcelToTable_conversion(r\"D:\\_1.Project\\AACM\\Pavement\\Excel Workbook\\Distress_Workbook\\E_distress_Remarks.xlsx\", \"Remarks_tbl\", \"Sheet1\")\n",
    "\n",
    "#creating point distress by select by attribute method, where chainage start and chainage end are equal\n",
    "arcpy.management.SelectLayerByAttribute(\"Keycode_tbl\", \"NEW_SELECTION\", '\"CHAINAGE_START\" = \"CHAINAGE_END\" ')\n",
    "arcpy.TableToTable_conversion(\"Keycode_tbl\", arcpy.env.workspace, \"P_Distress\")\n",
    "arcpy.management.SelectLayerByAttribute(\"Keycode_tbl\", \"CLEAR_SELECTION\")\n",
    "\n",
    "#creating Line distress by select by attribute method, where chainage start and end is not equal\n",
    "arcpy.management.SelectLayerByAttribute(\"Keycode_tbl\", \"NEW_SELECTION\", '\"CHAINAGE_START\" <> \"CHAINAGE_END\" ')\n",
    "arcpy.TableToTable_conversion(\"Keycode_tbl\", arcpy.env.workspace, \"L_Distress\")\n",
    "arcpy.management.SelectLayerByAttribute(\"Keycode_tbl\", \"CLEAR_SELECTION\")\n",
    "\n",
    "\n",
    "#creating feature point and line classes from point and line distess tables \n",
    "arcpy.management.XYTableToPoint(\"P_Distress\",\"Distress_P_wgs\",\"X\",\"Y\",\"\",arcpy.SpatialReference(4326))\n",
    "arcpy.management.XYToLine(\"L_Distress\",\"Distress_L_wgs\",\"X\",\"Y\",\"X_END\",\"Y_END\",\"GEODESIC\",\"\",arcpy.SpatialReference(4326),True)\n",
    "\n",
    "utm_crs = arcpy.SpatialReference(32640)\n",
    "arcpy.Project_management(\"Distress_P_wgs\",\"P_distress_utm\",utm_crs)\n",
    "arcpy.Project_management(\"Distress_L_wgs\",\"L_distress_utm\",utm_crs)\n",
    "\n",
    "\n",
    "print(GPS_P_filtered.head(10))\n",
    "print(\"Successfully completed!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below script will follow after snapping of distress line to pms sections\n",
    "* first, start and end vertexs of pms section will produced which used for splitting line distresses\n",
    "* lines distress will under go for splitting based on points produced from pms sections at start and end vertexes\n",
    "* Quantity value will be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "arcpy.env.workspace = r'D:\\_2.Programming\\_6.test\\test.gdb'\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "arcpy.management.AddFields(\"L_distress_utm\",[[\"section_occurence\",\"TEXT\",\"\",300],[\"split_len\",\"TEXT\"],[\"f_part\",\"TEXT\"],\n",
    "                                               [\"s_part\",\"TEXT\"],[\"qc\",\"TEXT\"],[\"Quantity\",\"DOUBLE\"],[\"Section_len\",\"DOUBLE\"]])\n",
    "\n",
    "arcpy.management.AddFields(\"P_distress_utm\",[[\"section_occurence\",\"TEXT\",\"\",300],[\"split_len\",\"TEXT\"],[ \"f_part\",\"TEXT\"],\n",
    "                                               [\"s_part\",\"TEXT\"],[\"qc\",\"TEXT\"],[\"Quantity\",\"DOUBLE\"],[\"Section_len\",\"DOUBLE\"]])    \n",
    "\n",
    "\n",
    "arcpy.management.CalculateField(\"L_distress_utm\",\"Section_len\", \"!SHAPE.LENGTH!\", \"PYTHON3\")\n",
    "\n",
    "expr = f\"!SURVEY_ID!+!COMMENT_!+!COMMENT_1!+!EVENT_DESC!+str(!CHAINAGE_START!)+str(!CHAINAGE_END!)+str(!LENGTH!)\"\n",
    " \n",
    "arcpy.management.CalculateField(\"L_distress_utm\",\"section_occurence\", expr, \"PYTHON3\")\n",
    "\n",
    "expr1 = f\"!SURVEY_ID!+!COMMENT_1!+!EVENT_DESC!+str(!CHAINAGE_START!)+str(!CHAINAGE_END!)+str(!LENGTH!)\"\n",
    "arcpy.management.CalculateField(\"P_distress_utm\",\"section_occurence\", expr1, \"PYTHON3\")\n",
    "\n",
    "arcpy.management.FeatureVerticesToPoints(\"PMS_Sections\",\"PMS_sec_Points\",\"BOTH_ENDS\")\n",
    "arcpy.management.SplitLineAtPoint(\"L_distress_utm\",\"L_GPS_Points\", \"Distress_L_split\",\"0.5 Meters\")\n",
    "\n",
    "\n",
    "\n",
    "def multiply_fields(a, b, c, d):\n",
    "    try:\n",
    "        a_value = float(a) if a is not None else 1.0\n",
    "        b_value = float(b) if b is not None else 1.0\n",
    "        c_value = float(c) if c is not None else 1.0\n",
    "        d_value = float(d) if d is not None else 1.0\n",
    "        result = a_value * b_value * c_value * d_value\n",
    "        return result\n",
    "    except (ValueError,TypeError):\n",
    "        return None\n",
    "    \n",
    "\n",
    "def multiply_p_distress(a, b, c):\n",
    "    try:\n",
    "        a_value = float(a) if a is not None else 1.0\n",
    "        b_value = float(b) if b is not None else 1.0\n",
    "        c_value = float(c) if c is not None else 1.0\n",
    "        result = a_value * b_value * c_value\n",
    "        return result\n",
    "    except (ValueError,TypeError):\n",
    "        return None\n",
    "    \n",
    "# Use a Search Cursor to count occurrences of each distress_L\n",
    "section_counter = Counter()\n",
    "with arcpy.da.SearchCursor(\"Distress_L_split\", [\"section_occurence\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        section_counter[row[0]] += 1\n",
    "\n",
    "\n",
    "with arcpy.da.UpdateCursor(\"Distress_L_split\", [\"section_occurence\", 'SHAPE@', \"COMMENT_1\", \"split_len\", \"f_part\",\n",
    "                                       \"s_part\",\"qc\",\"Quantity\",\"Section_len\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        same_sections = row[0]\n",
    "        if section_counter[same_sections] > 1 and '*' not in row[2]:      # distress length fall more than one section for linear distress\n",
    "            row[7] = (((row[1].length)/row[8]) * (float(row[2])))         # fallen length/distressL * ShapeLength\n",
    "        elif section_counter[same_sections] > 1 and '*' in row[2]:        # distress length fall in more than one section for area distress\n",
    "            row[4] = row[2].split(\"*\")[-1]                                # getting width of distress\n",
    "            row[7] = float(row[4]) * row[1].length                        # splited distress lenght multiply by width\n",
    "        elif section_counter[same_sections] == 1 and '*' not in row[2]:   # distress length fall in one section for linear distress\n",
    "            row[7] = float(row[2])\n",
    "        elif section_counter[same_sections] == 1 and '*' in row[2]:       # distress length fall in one section for area distress\n",
    "            row[4] = row[2].split(\"*\")[0]\n",
    "            row[5] = row[2].split(\"*\")[-1]\n",
    "            row[7] = float(row[4]) * float(row[5])  \n",
    "        else:\n",
    "            row[6] = \"check the values\"\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "with arcpy.da.UpdateCursor(\"P_distress_utm\", [\"COMMENT_1\", \"split_len\", \"f_part\", \"qc\", \"Quantity\", \"Section_len\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        if \"*\" in row[0]:\n",
    "            row[1] = row[0].split(\"*\")[0]\n",
    "            row[2] = row[0].split(\"*\")[-1]\n",
    "            row[4] = float(row[1]) * float(row[2])\n",
    "        elif \"*\" not in row[0]:\n",
    "            row[4] = float(row[0])\n",
    "        else:\n",
    "            row[3] = \"check the values\"\n",
    "        cursor.updateRow(row)\n",
    "     \n",
    "print (\"sucessfully completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below script will follow after splitting line distress's and calulation of quantity\n",
    "* convertig distress_L into midpoints\n",
    "* combine mid points of line distress and point distress to produced total or combine distresses\n",
    "* spatial join one to many with L_GPS_sections will carried out\n",
    "* a new field will be added for creating distress event desc and severity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = r'D:\\_2.Programming\\_6.test\\test.gdb'\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.management.FeatureVerticesToPoints(\"Distress_L_split\",\"Distress_L_midpoints\",\"MID\")\n",
    "\n",
    "arcpy.management.Merge([\"Distress_L_midpoints\",\"P_distress_utm\"],\"Distress_Combine\",\"\",'ADD_SOURCE_INFO')\n",
    "arcpy.analysis.SpatialJoin(\"L_GPS_Section\",\"Distress_Combine\",\"L_GPS_distress_Pts\",\"JOIN_ONE_TO_MANY\",\"KEEP_ALL\",\"\",\"INTERSECT\",\"0.5 METERS\")\n",
    "\n",
    "fields = arcpy.ListFields(\"L_GPS_distress_Pts\")\n",
    "arcpy.management.AddField(\"L_GPS_distress_Pts\",\"Event_distress\",\"TEXT\")\n",
    "\n",
    "\n",
    "with arcpy.da.UpdateCursor(\"L_GPS_distress_Pts\",[\"EVENT_DESC\",\"COMMENT\",\"Event_distress\",\"COMMENT_\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0]  == \"ALLIGATOR CRACKING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F1_L_Qty\"\n",
    "        elif row[0] == \"ALLIGATOR CRACKING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F1_M_Qty\"\n",
    "        elif row[0] == \"ALLIGATOR CRACKING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F1_H_Qty\"\n",
    "        elif row[0] == \"BLEEDING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F2_L_Qty\"\n",
    "        elif row[0] == \"BLEEDING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F2_M_Qty\"\n",
    "        elif row[0] == \"BLEEDING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F2_H_Qty\"\n",
    "        elif row[0] == \"BLOCK CRACKING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F3_L_Qty\"\n",
    "        elif row[0] == \"BLOCK CRACKING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F3_M_Qty\"\n",
    "        elif row[0] == \"BLOCK CRACKING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F3_H_Qty\"\n",
    "        elif row[0] == \"BUMPS & SAGS\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F4_L_Qty\"\n",
    "        elif row[0] == \"BUMPS & SAGS\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F4_M_Qty\"\n",
    "        elif row[0] == \"BUMPS & SAGS\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F4_H_Qty\"\n",
    "        elif row[0] == \"CORRUGATION\"  and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F5_L_Qty\"\n",
    "        elif row[0] == \"CORRUGATION\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F5_M_Qty\"\n",
    "        elif row[0] == \"CORRUGATION\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F5_H_Qty\"\n",
    "        elif row[0] == \"DEPRESSION\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F6_L_Qty\"\n",
    "        elif row[0] == \"DEPRESSION\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F6_M_Qty\"\n",
    "        elif row[0] == \"DEPRESSION\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F6_H_Qty\"\n",
    "        elif row[0] == \"EDGE CRACKING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F7_L_Qty\"\n",
    "        elif row[0] == \"EDGE CRACKING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F7_M_Qty\"\n",
    "        elif row[0] == \"EDGE CRACKING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F7_H_Qty\"\n",
    "        elif row[0] == \"JT REFLECTION CRACKING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F8_L_Qty\"\n",
    "        elif row[0] == \"JT REFLECTION CRACKING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F8_M_Qty\"\n",
    "        elif row[0] == \"JT REFLECTION CRACKING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F8_H_Qty\"\n",
    "        elif row[0] == \"LANE SHOULDER DROP OFF\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F9_L_Qty\"\n",
    "        elif row[0] == \"LANE SHOULDER DROP OFF\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F9_M_Qty\"\n",
    "        elif row[0] == \"LANE SHOULDER DROP OFF\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F9_H_Qty\"\n",
    "        elif row[0] == \"LONG & TRANS CRACKING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F10_L_Qty\"\n",
    "        elif row[0] == \"LONG & TRANS CRACKING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F10_M_Qty\"\n",
    "        elif row[0] == \"LONG & TRANS CRACKING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F10_H_Qty\"\n",
    "        elif row[0] == \"PATCHING & UTILITY CUT PATCHING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F11_L_Qty\"\n",
    "        elif row[0] == \"PATCHING & UTILITY CUT PATCHING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F11_M_Qty\"\n",
    "        elif row[0] == \"PATCHING & UTILITY CUT PATCHING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F11_H_Qty\"\n",
    "        elif row[0] == \"POLISHED AGGREGATE\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F12_L_Qty\"\n",
    "        #elif row[0] == \"POLISHED AGGREGATE\" and row[1] == \"MEDIUM\":\n",
    "            row[2] = \"F12_M_Qty\"\n",
    "        #elif row[0] == \"POLISHED AGGREGATE\" and row[1] == \"HIGH\":\n",
    "            row[2] = \"F12_H_Qty\"\n",
    "        elif row[0] == \"POTHOLES\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F13_L_Qty\"\n",
    "        elif row[0] == \"POTHOLES\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F13_M_Qty\"\n",
    "        elif row[0] == \"POTHOLES\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F13_H_Qty\"\n",
    "        elif row[0] == \"RAILROAD CROSSING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F14_L_Qty\"\n",
    "        elif row[0] == \"RAILROAD CROSSING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F14_M_Qty\"\n",
    "        elif row[0] == \"RAILROAD CROSSING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F14_H_Qty\"\n",
    "        elif row[0] == \"RUTTING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F15_L_Qty\"\n",
    "        elif row[0] == \"RUTTING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F15_M_Qty\"\n",
    "        elif row[0] == \"RUTTING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F15_H_Qty\"\n",
    "        elif row[0] == \"SHOVING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F16_L_Qty\"\n",
    "        elif row[0] == \"SHOVING\"  and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F16_M_Qty\"\n",
    "        elif row[0] == \"SHOVING\"  and  (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F16_H_Qty\"\n",
    "        elif row[0] == \"SLIPPAGE CRACKING\"  and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F17_L_Qty\"\n",
    "        elif row[0] == \"SLIPPAGE CRACKING\"  and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F17_M_Qty\"\n",
    "        elif row[0] == \"SPLIPPAGE CRACKING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F17_H_Qty\"\n",
    "        elif row[0] == \"UPHEAVIAL & SWELL\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F18_L_Qty\"\n",
    "        elif row[0] == \"UPHEAVIAL & SWELL\" and (row[1] == \"MEDIUM\" or row[3] ==  \"MEDIUM\"):\n",
    "            row[2] = \"F18_M_Qty\"\n",
    "        elif row[0] == \"UPHEAVIAL & SWELL\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F18_H_Qty\"\n",
    "        elif row[0] == \"RAVELLING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"):\n",
    "            row[2] = \"F19_M_Qty\"\n",
    "        elif row[0] == \"RAVELLING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"):\n",
    "            row[2] = \"F19_M_Qty\"\n",
    "        elif row[0] == \"RAVELLING\" and (row[1] == \"HIGH\" or row[3] == \"HIGH\"):\n",
    "            row[2] = \"F19_H_Qty\"\n",
    "        elif row[0] == \"WEATHERING\" and (row[1] == \"LOW\" or row[3] == \"LOW\"): \n",
    "            row[2] = \"F20_L_Qty\"\n",
    "        elif row[0] == \"WEATHERING\" and (row[1] == \"MEDIUM\" or row[3] == \"MEDIUM\"): \n",
    "            row[2] = \"F20_M_Qty\"\n",
    "        elif row[0] == \"WEATHERING\" and (row[1] == \"HIGH\" or row[3] ==  \"HIGH\"):\n",
    "            row[2] = \"F20_H_Qty\"\n",
    "        else:\n",
    "            row[2] = \"remarks\"\n",
    "        cursor.updateRow(row)\n",
    "print (\"sucessfully completed!!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below script will run after merging mid point and mid points distress and preparation of template for pivot\n",
    "* first, pivot table is created\n",
    "* summary of pivot table for sum based on SC_UUID will produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = r'D:\\_2.Programming\\_6.test\\test.gdb'\n",
    "feature_class = \"L_GPS_distress_Pts\"\n",
    "arcpy.management.PivotTable(feature_class,[\"OBJECTID\",\"OBJECTID\"],\"Event_distress\",\"Quantity\",\"pvt_tbl\")\n",
    "list_field = []\n",
    "tbl = arcpy.ListFields(\"pvt_tbl\")\n",
    "for field in tbl:\n",
    "    if field.name.startswith(\"F\"):\n",
    "        list_field.append(field.name) \n",
    "if list_field:\n",
    "    statistics_types = \"SUM\"\n",
    "    output_tbl = r'E:\\ADM\\Project\\mdb_processing\\Distress_Integration.gdb\\o_tbl_2'\n",
    "    statistics_fields = \";\".join([f\"{field} {statistics_types}\" for field in list_field])\n",
    "    arcpy.analysis.Statistics(\"pvt_tbl\", output_tbl, statistics_fields, \"SC_UUID\")\n",
    "\n",
    "    print(\"Statistics analysis completed.\")\n",
    "else:\n",
    "    print(\"No fields starting with 'F' found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below script will follow after production of summary table\n",
    "* The output of this below step will forwarded for paver in the form of shapefile\n",
    "* PMS sections on the template of paver will imported\n",
    "* Join paver to summary table and caclulate distress's and remove join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.env.workspace = r'D:\\_2.Programming\\_6.test\\test.gdb'\n",
    "feature_class = 'paver_file'\n",
    "table = 'o_tbl_2'\n",
    "\n",
    "# Define the common field\n",
    "common_field = 'SC_UUID'\n",
    "\n",
    "summary_fields = [field.name for field in arcpy.ListFields(table) if field.name.startswith(\"SUM\")]\n",
    "summary_to_paver = [field.replace(\"SUM_\", \"\") for field in summary_fields]\n",
    "\n",
    "# Define the fields to be calculated\n",
    "fields_to_calculate = summary_to_paver\n",
    "\n",
    "# Join the table to the feature class\n",
    "arcpy.management.AddJoin(feature_class, common_field, table, common_field)\n",
    "\n",
    "# Calculate fields in the feature class based on the corresponding fields in the table\n",
    "for field in fields_to_calculate:\n",
    "    expression = '!{}!'.format('o_tbl_2.SUM_' + field)\n",
    "    arcpy.management.CalculateField(feature_class, field, expression, 'PYTHON3')\n",
    "\n",
    "# Remove the join\n",
    "arcpy.management.RemoveJoin(feature_class)\n",
    "\n",
    "print(\"Fields calculated successfully.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
